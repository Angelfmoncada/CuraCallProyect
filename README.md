# Gemma3 Voice Chat ğŸ¤ğŸ¤–

Un chatbot completo con capacidades de voz que utiliza **Ollama (Gemma3:4B)**, **FastAPI**, **Next.js** y **Web Speech API**. Permite conversaciones por voz y texto con respuestas en tiempo real mediante streaming.

## ğŸš€ CaracterÃ­sticas

- ğŸ™ï¸ **Reconocimiento de voz** (Speech-to-Text) usando Web Speech API
- ğŸ”Š **SÃ­ntesis de voz** (Text-to-Speech) para respuestas del asistente
- âš¡ **Streaming en tiempo real** de respuestas del modelo
- ğŸ¨ **Interfaz moderna** con TailwindCSS y Framer Motion
- ğŸ¤– **Modelo Gemma3:4B** ejecutÃ¡ndose localmente con Ollama
- ğŸ“± **DiseÃ±o responsive** que funciona en desktop y mÃ³vil

## ğŸ“‹ Requisitos

- **Node.js** 18+ 
- **Python** 3.8+
- **Ollama** instalado y ejecutÃ¡ndose

## ğŸ› ï¸ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone <repository-url>
cd curacall11
```

### 2. Configurar Ollama

```bash
# Instalar Ollama (si no estÃ¡ instalado)
curl -fsSL https://ollama.ai/install.sh | sh

# Iniciar el servicio
ollama serve

# Descargar el modelo Gemma3:4B
ollama pull gemma3:4b
```

### 3. Configurar el Backend

```bash
cd server

# Instalar dependencias
pip install -U pip
pip install -e . || pip install fastapi uvicorn[standard] pydantic ollama

# Copiar variables de entorno (opcional)
cp .env.example .env

# Iniciar el servidor
uvicorn main:app --reload --port 8000
```

El backend estarÃ¡ disponible en `http://localhost:8000`

### 4. Configurar el Frontend

```bash
cd ../web

# Instalar dependencias
npm install

# Iniciar el servidor de desarrollo
npm run dev
```

El frontend estarÃ¡ disponible en `http://localhost:3000`

## ğŸ¯ Uso

1. **Abrir la aplicaciÃ³n** en `http://localhost:3000`
2. **Escribir un mensaje** en el campo de texto o **hacer clic en el micrÃ³fono** para usar reconocimiento de voz
3. **Enviar el mensaje** y ver la respuesta en tiempo real
4. **Escuchar la respuesta** (se reproduce automÃ¡ticamente si el navegador lo permite)

### Navegadores Compatibles

- **Chrome/Edge**: Soporte completo para voz
- **Firefox**: Funcionalidad limitada de voz
- **Safari**: Soporte parcial

> **Nota**: Si el reconocimiento de voz no funciona, puedes usar el teclado normalmente.

## ğŸ—ï¸ Estructura del Proyecto

```
curacall11/
â”œâ”€â”€ server/                 # Backend FastAPI
â”‚   â”œâ”€â”€ main.py            # AplicaciÃ³n principal
â”‚   â”œâ”€â”€ pyproject.toml     # Dependencias Python
â”‚   â”œâ”€â”€ .env.example       # Variables de entorno
â”‚   â””â”€â”€ README.md          # DocumentaciÃ³n del backend
â”œâ”€â”€ web/                   # Frontend Next.js
â”‚   â”œâ”€â”€ app/               # App Router de Next.js
â”‚   â”‚   â”œâ”€â”€ components/    # Componentes React
â”‚   â”‚   â”œâ”€â”€ layout.tsx     # Layout principal
â”‚   â”‚   â””â”€â”€ page.tsx       # PÃ¡gina principal
â”‚   â”œâ”€â”€ styles/            # Estilos globales
â”‚   â”œâ”€â”€ package.json       # Dependencias Node.js
â”‚   â””â”€â”€ *.config.*         # Archivos de configuraciÃ³n
â””â”€â”€ README.md              # Este archivo
```

## ğŸ”§ API Endpoints

### Backend (Puerto 8000)

- `GET /api/health` - Estado del servidor
- `GET /api/models` - Modelos disponibles en Ollama
- `POST /api/chat` - Chat sin streaming
- `POST /api/chat/stream` - Chat con streaming (usado por el frontend)

### Ejemplo de uso de la API

```bash
# Probar el endpoint de salud
curl http://localhost:8000/api/health

# Enviar un mensaje
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hola, Â¿cÃ³mo estÃ¡s?", "model": "gemma3:4b"}'
```

## ğŸ³ Docker (Opcional)

### Backend

```bash
cd server
docker build -t gemma3-chat-backend .
docker run -p 8000:8000 gemma3-chat-backend
```

### Frontend

```bash
cd web
docker build -t gemma3-chat-frontend .
docker run -p 3000:3000 gemma3-chat-frontend
```

## ğŸ› ï¸ Desarrollo

### Comandos Ãºtiles

```bash
# Backend
cd server
uvicorn main:app --reload --port 8000  # Desarrollo
uvicorn main:app --host 127.0.0.1 --port 8000  # ProducciÃ³n

# Frontend
cd web
npm run dev        # Desarrollo
npm run build      # Construir para producciÃ³n
npm run start      # Ejecutar versiÃ³n de producciÃ³n
npm run lint       # Linter
```

### Variables de Entorno

**Backend** (`server/.env`):
```env
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=gemma3:4b
```

**Frontend** (opcional):
```env
NEXT_PUBLIC_API_BASE=http://localhost:8000
```

## ğŸ” SoluciÃ³n de Problemas

### Ollama no responde
- Verificar que Ollama estÃ© ejecutÃ¡ndose: `ollama serve`
- Verificar que el modelo estÃ© descargado: `ollama list`
- Probar el modelo: `ollama run gemma3:4b "Hola"`

### Error de CORS
- El backend ya incluye configuraciÃ³n CORS para desarrollo
- Para producciÃ³n, ajustar los orÃ­genes permitidos en `server/main.py`

### Reconocimiento de voz no funciona
- Usar Chrome o Edge para mejor compatibilidad
- Verificar permisos del micrÃ³fono en el navegador
- Usar HTTPS en producciÃ³n (requerido por Web Speech API)

### Puerto ocupado
```bash
# Cambiar puerto del backend
uvicorn main:app --port 8001

# Cambiar puerto del frontend
npm run dev -- --port 3001
```

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la licencia MIT.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crear una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir un Pull Request

---

**Â¡Disfruta conversando con Gemma3! ğŸ‰**